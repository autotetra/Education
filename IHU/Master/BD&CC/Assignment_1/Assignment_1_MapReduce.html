{ "cells": [ { "cell_type": "markdown", "metadata": {}, "source": [ "# Write
your full name here\n", "# Assignment 1: MapReduce - Blog Posts and Comments" ]
}, { "cell_type": "markdown", "metadata": {}, "source": [ "## Input file
description\n", "\n", "You are given the `comments.csv` input file. It is a
Comma Separated Values (CSV) file that stores comment relationships. More
specifically, the file includes 1 million rows and each row stores a tuple of
the form:\n", "\n", "`PostAuthor,CommentAuthor,CommentDate`\n", "\n", "The tuple
contains three fields (columns):\n", "\n", "* `PostAuthor`: this is a blog user
who authored a blog post.\n", "* `CommentAuthor`: this is another user who has
commented on the post of the PostAuthor.\n", "* `CommentDate`: this field stores
the date that the comment was made.\n" ] }, { "cell_type": "markdown",
"metadata": {}, "source": [ "## Tasks\n", "\n", "You will write 3 MapReduce jobs
**here** by using MRJob:\n", "\n", "* The first job will scan the input file and
for each `PostAuthor` it will construct a tuple that contains:\n", " - The
number of comments made to *all* of his/her posts, and\n", " - A list of the
comments in the form `(CommentAuthor,CommentDate)`. The list must be sorted in
decreasing `CommentDate` order. Namely, the most recent comment must be placed
at the top, followed by the older ones.\n", " - Example output: `PostAuthor
NumberofComments
[(Commentator,Date)(Commentator,Date)(Commentator,Date)()...]`\n", "\n", "* The
second job will scan the input file and for each `PostAuthor` it will construct
a tuple that contains:\n", " - The number of the *distinct* commentators who
made a comment to *all* of his/her posts, and\n", " - A list of the commentators
in the form `[(DistinctCommentator1)(DistinctCommentator2)...]`\n", " - Example
output: `PostAuthor NumberofDistrinctCommentators
[(DistinctCommentator1)(DistinctCommentator2)...]`\n", " - Use a combiner
here.\n", "\n", "* The third job will scan the input file and for each
`CommentAuthor` it will construct a tuple that contains:\n", " - The number of
comments that `CommentAuthor` has made to *all* posts, and\n", " - A list of the
comments in the form `(PostAuthor,CommentDate)`. The list must be sorted in
decreasing `CommentDate` order. Namely, the most recent comment must be placed
at the top, followed by the older ones.\n", " - Example output: `CommentAuthor
NumberofComments [(PostAuthor,Date)(PostAuthor,Date)(PostAuthor,Date)()...]`\n"
] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## Deliverables\n",
"\n", "**There will be a single deliverable, this notebook**. You will organize
your answers according to the provided structure, which is identical to the
example notebooks that were uploaded to the e-learning platform. **Please write
your full name in both the notebook's filename and the notebook's title (first
line of first cell)**.\n", "\n", "Then, upload the file in the e-learning
platform.\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "##
Answer to task 1" ] }, { "cell_type": "markdown", "metadata": {}, "source": [
"### 1.1 Python code for MapReduce" ] }, { "cell_type": "code",
"execution_count": null, "metadata": {}, "outputs": [], "source": [ "%%file
task1.py\n", "#!/usr/bin/env python3\n", "\n", "from mrjob.job import MRJob\n" ]
}, { "cell_type": "markdown", "metadata": {}, "source": [ "### 1.2 Standalone
execution" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### 1.3
Running in the Hadoop cluster in a fully/pseudo distributed mode" ] }, {
"cell_type": "markdown", "metadata": {}, "source": [ "### 1.4. Copy the output
file from HDFS to the local file system" ] }, { "cell_type": "markdown",
"metadata": {}, "source": [ "## Answer to task 2" ] }, { "cell_type":
"markdown", "metadata": {}, "source": [ "### 2.1 Python code for MapReduce" ] },
{ "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [],
"source": [ "%%file task2.py\n", "#!/usr/bin/env python3\n", "\n", "from
mrjob.job import MRJob" ] }, { "cell_type": "markdown", "metadata": {},
"source": [ "### 2.2 Standalone execution" ] }, { "cell_type": "markdown",
"metadata": {}, "source": [ "### 2.3 Running in the Hadoop cluster in a
fully/pseudo distributed mode" ] }, { "cell_type": "markdown", "metadata": {},
"source": [ "### 2.4. Copy the output file from HDFS to the local file system" ]
}, { "cell_type": "markdown", "metadata": {}, "source": [ "## Answer to task 3"
] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### 3.1 Python code
for MapReduce" ] }, { "cell_type": "code", "execution_count": null, "metadata":
{}, "outputs": [], "source": [ "%%file task3.py\n", "#!/usr/bin/env python3\n",
"\n", "from mrjob.job import MRJob" ] }, { "cell_type": "markdown", "metadata":
{}, "source": [ "### 3.2 Standalone execution" ] }, { "cell_type": "markdown",
"metadata": {}, "source": [ "### 3.3 Running in the Hadoop cluster in a
fully/pseudo distributed mode" ] }, { "cell_type": "markdown", "metadata": {},
"source": [ "### 3.4. Copy the output file from HDFS to the local file system" ]
} ], "metadata": { "kernelspec": { "display_name": "Python 3", "language":
"python", "name": "python3" }, "language_info": { "codemirror_mode": { "name":
"ipython", "version": 3 }, "file_extension": ".py", "mimetype": "text/x-python",
"name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3",
"version": "3.8.3" } }, "nbformat": 4, "nbformat_minor": 5 }
