{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9525042",
   "metadata": {},
   "source": [
    "# Write your full name here\n",
    "# Assignment 1: MapReduce - Blog Posts and Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e75f9c",
   "metadata": {},
   "source": [
    "## Input file description\n",
    "\n",
    "You are given the `comments.csv` input file. It is a Comma Separated Values (CSV) file that stores comment relationships. More specifically, the file includes 1 million rows and each row stores a tuple of the form:\n",
    "\n",
    "`PostAuthor,CommentAuthor,CommentDate`\n",
    "\n",
    "The tuple contains three fields (columns):\n",
    "\n",
    "* `PostAuthor`: this is a blog user who authored a blog post.\n",
    "* `CommentAuthor`: this is another user who has commented on the post of the PostAuthor.\n",
    "* `CommentDate`: this field stores the date that the comment was made.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd65cb65",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "You will write 3 MapReduce jobs **here** by using MRJob:\n",
    "\n",
    "* The first job will scan the input file and for each `PostAuthor` it will construct a tuple that contains:\n",
    " - The number of comments made to *all* of his/her posts, and\n",
    " - A list of the comments in the form `(CommentAuthor,CommentDate)`. The list must be sorted in decreasing `CommentDate` order. Namely, the most recent comment must be placed at the top, followed by the older ones.\n",
    " - Example output: `PostAuthor NumberofComments [(Commentator,Date)(Commentator,Date)(Commentator,Date)()...]`\n",
    "\n",
    "* The second job will scan the input file and for each `PostAuthor` it will construct a tuple that contains:\n",
    " - The number of the *distinct* commentators who made a comment to *all* of his/her posts, and\n",
    " - A list of the commentators in the form `[(DistinctCommentator1)(DistinctCommentator2)...]`\n",
    " - Example output: `PostAuthor NumberofDistrinctCommentators [(DistinctCommentator1)(DistinctCommentator2)...]`\n",
    " - Use a combiner here.\n",
    "\n",
    "* The third job will scan the input file and for each `CommentAuthor` it will construct a tuple that contains:\n",
    " - The number of comments that `CommentAuthor` has made to *all* posts, and\n",
    " - A list of the comments in the form `(PostAuthor,CommentDate)`. The list must be sorted in decreasing `CommentDate` order. Namely, the most recent comment must be placed at the top, followed by the older ones.\n",
    " - Example output: `CommentAuthor NumberofComments [(PostAuthor,Date)(PostAuthor,Date)(PostAuthor,Date)()...]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4391a882",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "\n",
    "**There will be a single deliverable, this notebook**. You will organize your answers according to the provided structure, which is identical to the example notebooks that were uploaded to the e-learning platform. **Please write your full name in both the notebook's filename and the notebook's title (first line of first cell)**.\n",
    "\n",
    "Then, upload the file in the e-learning platform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef5269d",
   "metadata": {},
   "source": [
    "## Answer to task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a998d1f",
   "metadata": {},
   "source": [
    "### 1.1 Python code for MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c56ab55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task1.py\n"
     ]
    }
   ],
   "source": [
    "%%file task1.py\n",
    "from mrjob.job import MRJob\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "class PostAuthorComments(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        # Parse the line using csv\n",
    "        reader = csv.reader([line])\n",
    "        for row in reader:\n",
    "            post_author, comment_author, comment_date = row\n",
    "            yield post_author, (comment_author, comment_date)\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        comments = list(values)\n",
    "        comments.sort(key=lambda x: datetime.strptime(x[1], \"%Y-%m-%d\"), reverse=True)\n",
    "        yield key, (len(comments), comments)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    PostAuthorComments.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87997f08",
   "metadata": {},
   "source": [
    "### 1.2 Standalone execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b142e1a9-33f1-41d5-9884-e9fb5ce27c4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/task1.hadoop.20240421.185916.532951\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/task1.hadoop.20240421.185916.532951/output\n",
      "Streaming final output from /tmp/task1.hadoop.20240421.185916.532951/output...\n",
      "Removing temp directory /tmp/task1.hadoop.20240421.185916.532951...\n"
     ]
    }
   ],
   "source": [
    "!python task1.py comments.csv > job_1_output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce87f38",
   "metadata": {},
   "source": [
    "### 1.3 Running in the Hadoop cluster in a fully/pseudo distributed mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c652187-799f-45f1-a8aa-676e786f2a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 items\n",
      "drwxr-xr-x   - hadoop supergroup          0 2024-02-28 23:34 /user/hadoop/.sparkStaging\n",
      "-rw-r--r--   1 hadoop supergroup   25981917 2024-04-21 22:36 /user/hadoop/comments.csv\n",
      "drwxr-xr-x   - dr.who supergroup          0 2024-04-09 21:20 /user/hadoop/ihu\n",
      "drwxr-xr-x   - hadoop supergroup          0 2024-04-21 13:45 /user/hadoop/out_iiDL22\n",
      "drwxr-xr-x   - hadoop supergroup          0 2024-02-28 23:31 /user/hadoop/out_iiPI\n",
      "drwxr-xr-x   - hadoop supergroup          0 2024-02-28 23:30 /user/hadoop/out_iiTL\n",
      "-rw-r--r--   1 hadoop supergroup        889 2024-04-21 21:28 /user/hadoop/sample_data.csv\n",
      "drwxr-xr-x   - hadoop supergroup          0 2024-02-28 23:35 /user/hadoop/selected_df.csv\n",
      "-rw-r--r--   1 dr.who supergroup      99993 2024-02-23 01:26 /user/hadoop/shakespear_input.txt\n",
      "drwxr-xr-x   - hadoop supergroup          0 2024-02-22 12:45 /user/hadoop/tmp\n",
      "-rw-r--r--   1 hadoop supergroup      61947 2024-02-28 23:35 /user/hadoop/universities_ranking.csv\n",
      "-rw-r--r--   1 hadoop supergroup      33500 2024-02-28 23:35 /user/hadoop/universities_ranking_b.csv\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /user/hadoop/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6b3f8a4-29b8-4771-ab24-6c0fd3d17493",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -put comments.csv /user/hadoop/comments.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f571f709-8953-4e7c-bfb2-3bdc30bd030a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /home/hadoop/hadoop/bin...\n",
      "Found hadoop binary: /home/hadoop/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /home/hadoop/hadoop...\n",
      "Found Hadoop streaming jar: /home/hadoop/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/task1.hadoop.20240421.193716.124821\n",
      "uploading working dir files to hdfs:///user/hadoop/tmp/mrjob/task1.hadoop.20240421.193716.124821/files/wd...\n",
      "Copying other local files to hdfs:///user/hadoop/tmp/mrjob/task1.hadoop.20240421.193716.124821/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar7858821130576483077/] [] /tmp/streamjob8013802196787040630.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1713715981079_0003\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1713715981079_0003\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1713715981079_0003\n",
      "  The url to track the job: http://BDCC:8088/proxy/application_1713715981079_0003/\n",
      "  Running job: job_1713715981079_0003\n",
      "  Job job_1713715981079_0003 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 35% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 81%\n",
      "   map 100% reduce 89%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1713715981079_0003 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/job_1_output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=25986013\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=26007362\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=35981923\n",
      "\t\tFILE: Number of bytes written=72808060\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=25986209\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=26007362\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=83171328\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=33312768\n",
      "\t\tTotal time spent by all map tasks (ms)=81222\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=81222\n",
      "\t\tTotal time spent by all reduce tasks (ms)=32532\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=32532\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=81222\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=32532\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=35150\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=848\n",
      "\t\tInput split bytes=196\n",
      "\t\tMap input records=1000000\n",
      "\t\tMap output bytes=33981917\n",
      "\t\tMap output materialized bytes=35981929\n",
      "\t\tMap output records=1000000\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=279093248\n",
      "\t\tPeak Map Virtual memory (bytes)=2692341760\n",
      "\t\tPeak Reduce Physical memory (bytes)=218963968\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2699403264\n",
      "\t\tPhysical memory (bytes) snapshot=706478080\n",
      "\t\tReduce input groups=2714\n",
      "\t\tReduce input records=1000000\n",
      "\t\tReduce output records=2714\n",
      "\t\tReduce shuffle bytes=35981929\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=2000000\n",
      "\t\tTotal committed heap usage (bytes)=426659840\n",
      "\t\tVirtual memory (bytes) snapshot=7955750912\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/hadoop/job_1_output\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/task1.hadoop.20240421.193716.124821...\n",
      "Removing temp directory /tmp/task1.hadoop.20240421.193716.124821...\n"
     ]
    }
   ],
   "source": [
    "!python task1.py -r hadoop hdfs:///user/hadoop/comments.csv --output-dir hdfs:///user/hadoop/job_1_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ad97a5",
   "metadata": {},
   "source": [
    "### 1.4. Copy the output file from HDFS to the local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7c40369-ee40-4aaf-842d-891dcb1addcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 hadoop supergroup          0 2024-04-21 22:39 /user/hadoop/job_1_output/_SUCCESS\n",
      "-rw-r--r--   1 hadoop supergroup   26007362 2024-04-21 22:39 /user/hadoop/job_1_output/part-00000\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /user/hadoop/job_1_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8cc5c525-0dbb-403c-a653-8ce724d5108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -copyToLocal /user/hadoop/job_1_output/part-00000 job_1_output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a943897c",
   "metadata": {},
   "source": [
    "## Answer to task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5b98e7",
   "metadata": {},
   "source": [
    "### 2.1 Python code for MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb1f5900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing task2.py\n"
     ]
    }
   ],
   "source": [
    "%%file task2.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import csv\n",
    "\n",
    "class DistinctCommentators(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_comments,\n",
    "                   reducer=self.reducer_count_comments),\n",
    "            MRStep(reducer=self.reducer_list_commentators)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_comments(self, _, line):\n",
    "        # Parse the line using csv\n",
    "        reader = csv.reader([line])\n",
    "        for row in reader:\n",
    "            post_author, comment_author, _ = row\n",
    "            yield (post_author, comment_author), None\n",
    "\n",
    "    def reducer_count_comments(self, author_pair, _):\n",
    "        post_author, comment_author = author_pair\n",
    "        yield post_author, comment_author\n",
    "\n",
    "    def reducer_list_commentators(self, post_author, commentators):\n",
    "        unique_commentators = set(commentators)\n",
    "        yield post_author, (len(unique_commentators), list(unique_commentators))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    DistinctCommentators.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d19aebb",
   "metadata": {},
   "source": [
    "### 2.2 Standalone execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d314f127-f0ab-4732-a8b8-95f4775cf0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/task2.hadoop.20240421.200317.024797\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/task2.hadoop.20240421.200317.024797/output\n",
      "Streaming final output from /tmp/task2.hadoop.20240421.200317.024797/output...\n",
      "Removing temp directory /tmp/task2.hadoop.20240421.200317.024797...\n"
     ]
    }
   ],
   "source": [
    "!python task2.py comments.csv > job_2_output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ed6fab",
   "metadata": {},
   "source": [
    "### 2.3 Running in the Hadoop cluster in a fully/pseudo distributed mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad3a99c-9bc5-48ad-8b70-70df6dc42974",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -put comments.csv /user/hadoop/comments.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3aa87ca-5846-4439-bc1c-6dd5f9322bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /home/hadoop/hadoop/bin...\n",
      "Found hadoop binary: /home/hadoop/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /home/hadoop/hadoop...\n",
      "Found Hadoop streaming jar: /home/hadoop/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/task2.hadoop.20240421.200450.732527\n",
      "uploading working dir files to hdfs:///user/hadoop/tmp/mrjob/task2.hadoop.20240421.200450.732527/files/wd...\n",
      "Copying other local files to hdfs:///user/hadoop/tmp/mrjob/task2.hadoop.20240421.200450.732527/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar6937329494835278535/] [] /tmp/streamjob4904205534611988525.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1713715981079_0004\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1713715981079_0004\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1713715981079_0004\n",
      "  The url to track the job: http://BDCC:8088/proxy/application_1713715981079_0004/\n",
      "  Running job: job_1713715981079_0004\n",
      "  Job job_1713715981079_0004 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 32% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1713715981079_0004 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/tmp/mrjob/task2.hadoop.20240421.200450.732527/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=25986013\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=196362\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=27981923\n",
      "\t\tFILE: Number of bytes written=56808210\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=25986209\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=196362\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=86914048\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=15800320\n",
      "\t\tTotal time spent by all map tasks (ms)=84877\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=84877\n",
      "\t\tTotal time spent by all reduce tasks (ms)=15430\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=15430\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=84877\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=15430\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=20750\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=753\n",
      "\t\tInput split bytes=196\n",
      "\t\tMap input records=1000000\n",
      "\t\tMap output bytes=25981917\n",
      "\t\tMap output materialized bytes=27981929\n",
      "\t\tMap output records=1000000\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=275955712\n",
      "\t\tPeak Map Virtual memory (bytes)=2693197824\n",
      "\t\tPeak Reduce Physical memory (bytes)=168239104\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2657017856\n",
      "\t\tPhysical memory (bytes) snapshot=677945344\n",
      "\t\tReduce input groups=10920\n",
      "\t\tReduce input records=1000000\n",
      "\t\tReduce output records=10920\n",
      "\t\tReduce shuffle bytes=27981929\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=2000000\n",
      "\t\tTotal committed heap usage (bytes)=398663680\n",
      "\t\tVirtual memory (bytes) snapshot=7957995520\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar5085033766413140558/] [] /tmp/streamjob1698000903625967965.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1713715981079_0005\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1713715981079_0005\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1713715981079_0005\n",
      "  The url to track the job: http://BDCC:8088/proxy/application_1713715981079_0005/\n",
      "  Running job: job_1713715981079_0005\n",
      "  Job job_1713715981079_0005 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1713715981079_0005 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/job_2_output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=200458\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=149446\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=218208\n",
      "\t\tFILE: Number of bytes written=1280570\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=200776\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=149446\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=37523456\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=9722880\n",
      "\t\tTotal time spent by all map tasks (ms)=36644\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=36644\n",
      "\t\tTotal time spent by all reduce tasks (ms)=9495\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=9495\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=36644\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=9495\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2990\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=713\n",
      "\t\tInput split bytes=318\n",
      "\t\tMap input records=10920\n",
      "\t\tMap output bytes=196362\n",
      "\t\tMap output materialized bytes=218214\n",
      "\t\tMap output records=10920\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=251060224\n",
      "\t\tPeak Map Virtual memory (bytes)=2652557312\n",
      "\t\tPeak Reduce Physical memory (bytes)=148946944\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2652856320\n",
      "\t\tPhysical memory (bytes) snapshot=649412608\n",
      "\t\tReduce input groups=2714\n",
      "\t\tReduce input records=10920\n",
      "\t\tReduce output records=2714\n",
      "\t\tReduce shuffle bytes=218214\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=21840\n",
      "\t\tTotal committed heap usage (bytes)=398663680\n",
      "\t\tVirtual memory (bytes) snapshot=7955718144\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/hadoop/job_2_output\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/task2.hadoop.20240421.200450.732527...\n",
      "Removing temp directory /tmp/task2.hadoop.20240421.200450.732527...\n"
     ]
    }
   ],
   "source": [
    "!python task2.py -r hadoop hdfs:///user/hadoop/comments.csv --output-dir hdfs:///user/hadoop/job_2_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555659f3",
   "metadata": {},
   "source": [
    "### 2.4. Copy the output file from HDFS to the local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4cb085b-27b5-46d6-aa9c-c25e6d76e5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 hadoop supergroup          0 2024-04-21 23:07 /user/hadoop/job_2_output/_SUCCESS\n",
      "-rw-r--r--   1 hadoop supergroup     149446 2024-04-21 23:07 /user/hadoop/job_2_output/part-00000\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /user/hadoop/job_2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b995bba4-35ce-480d-a033-ced01b4cc587",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -copyToLocal /user/hadoop/job_2_output/part-00000 job_2_output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965f8cab",
   "metadata": {},
   "source": [
    "## Answer to task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbe17b2",
   "metadata": {},
   "source": [
    "### 3.1 Python code for MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a61c4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing task3.py\n"
     ]
    }
   ],
   "source": [
    "%%file task3.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "class CommentAuthorActivity(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        # Parse the line using csv\n",
    "        reader = csv.reader([line])\n",
    "        for row in reader:\n",
    "            post_author, comment_author, comment_date = row\n",
    "            yield comment_author, (post_author, comment_date)\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        # Sort comments by date in descending order\n",
    "        sorted_comments = sorted(values, key=lambda x: datetime.strptime(x[1], \"%Y-%m-%d\"), reverse=True)\n",
    "        yield key, (len(sorted_comments), list(sorted_comments))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    CommentAuthorActivity.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45592aa",
   "metadata": {},
   "source": [
    "### 3.2 Standalone execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "264880be-9606-4b90-86ed-3af7335ee553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/task3.hadoop.20240421.203405.968257\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/task3.hadoop.20240421.203405.968257/output\n",
      "Streaming final output from /tmp/task3.hadoop.20240421.203405.968257/output...\n",
      "Removing temp directory /tmp/task3.hadoop.20240421.203405.968257...\n"
     ]
    }
   ],
   "source": [
    "!python task3.py comments.csv > job_3_output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885ebd2e",
   "metadata": {},
   "source": [
    "### 3.3 Running in the Hadoop cluster in a fully/pseudo distributed mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3341ae79-7f0f-4170-813d-6f905de2f802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /home/hadoop/hadoop/bin...\n",
      "Found hadoop binary: /home/hadoop/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /home/hadoop/hadoop...\n",
      "Found Hadoop streaming jar: /home/hadoop/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/task3.hadoop.20240421.203816.775457\n",
      "uploading working dir files to hdfs:///user/hadoop/tmp/mrjob/task3.hadoop.20240421.203816.775457/files/wd...\n",
      "Copying other local files to hdfs:///user/hadoop/tmp/mrjob/task3.hadoop.20240421.203816.775457/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar7036806997918749844/] [] /tmp/streamjob7397435104422057290.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1713715981079_0006\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1713715981079_0006\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1713715981079_0006\n",
      "  The url to track the job: http://BDCC:8088/proxy/application_1713715981079_0006/\n",
      "  Running job: job_1713715981079_0006\n",
      "  Job job_1713715981079_0006 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 29% reduce 0%\n",
      "   map 60% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 83%\n",
      "   map 100% reduce 93%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1713715981079_0006 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/job_3_output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=25986013\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=26066459\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=35981923\n",
      "\t\tFILE: Number of bytes written=72808060\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=25986209\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=26066459\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=86372352\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=31157248\n",
      "\t\tTotal time spent by all map tasks (ms)=84348\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=84348\n",
      "\t\tTotal time spent by all reduce tasks (ms)=30427\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=30427\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=84348\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=30427\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=32330\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=896\n",
      "\t\tInput split bytes=196\n",
      "\t\tMap input records=1000000\n",
      "\t\tMap output bytes=33981917\n",
      "\t\tMap output materialized bytes=35981929\n",
      "\t\tMap output records=1000000\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=280879104\n",
      "\t\tPeak Map Virtual memory (bytes)=2693505024\n",
      "\t\tPeak Reduce Physical memory (bytes)=220954624\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2696847360\n",
      "\t\tPhysical memory (bytes) snapshot=699805696\n",
      "\t\tReduce input groups=2712\n",
      "\t\tReduce input records=1000000\n",
      "\t\tReduce output records=2712\n",
      "\t\tReduce shuffle bytes=35981929\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=2000000\n",
      "\t\tTotal committed heap usage (bytes)=426672128\n",
      "\t\tVirtual memory (bytes) snapshot=7955492864\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/hadoop/job_3_output\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/task3.hadoop.20240421.203816.775457...\n",
      "Removing temp directory /tmp/task3.hadoop.20240421.203816.775457...\n"
     ]
    }
   ],
   "source": [
    "!python task3.py -r hadoop hdfs:///user/hadoop/comments.csv --output-dir hdfs:///user/hadoop/job_3_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747e9936",
   "metadata": {},
   "source": [
    "### 3.4. Copy the output file from HDFS to the local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b108333e-30d8-4238-a35d-f6c8d9f1cff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 hadoop supergroup          0 2024-04-21 23:41 /user/hadoop/job_3_output/_SUCCESS\n",
      "-rw-r--r--   1 hadoop supergroup   26066459 2024-04-21 23:41 /user/hadoop/job_3_output/part-00000\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /user/hadoop/job_3_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5ee4f0-12b8-423c-a2fb-4b605e024b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -cat /user/hadoop/job_3_output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca9cb0ef-c67d-4294-bc64-2c13e273753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -copyToLocal /user/hadoop/job_3_output/part-00000 job_3_output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37898e4e-a032-42bd-a682-8ce7cc4bcee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
