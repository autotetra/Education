{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write your full name here\n",
    "# Assignment 1: MapReduce - Blog Posts and Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input file description\n",
    "\n",
    "You are given the `comments.csv` input file. It is a Comma Separated Values (CSV) file that stores comment relationships. More specifically, the file includes 1 million rows and each row stores a tuple of the form:\n",
    "\n",
    "`PostAuthor,CommentAuthor,CommentDate`\n",
    "\n",
    "The tuple contains three fields (columns):\n",
    "\n",
    "* `PostAuthor`: this is a blog user who authored a blog post.\n",
    "* `CommentAuthor`: this is another user who has commented on the post of the PostAuthor.\n",
    "* `CommentDate`: this field stores the date that the comment was made.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ec5858",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "You will write 3 MapReduce jobs **here** by using MRJob:\n",
    "\n",
    "* The first job will scan the input file and for each `PostAuthor` it will construct a tuple that contains:\n",
    " - The number of comments made to *all* of his/her posts, and\n",
    " - A list of the comments in the form `(CommentAuthor,CommentDate)`. The list must be sorted in decreasing `CommentDate` order. Namely, the most recent comment must be placed at the top, followed by the older ones.\n",
    " - Example output: `PostAuthor NumberofComments [(Commentator,Date)(Commentator,Date)(Commentator,Date)()...]`\n",
    "\n",
    "* The second job will scan the input file and for each `PostAuthor` it will construct a tuple that contains:\n",
    " - The number of the *distinct* commentators who made a comment to *all* of his/her posts, and\n",
    " - A list of the commentators in the form `[(DistinctCommentator1)(DistinctCommentator2)...]`\n",
    " - Example output: `PostAuthor NumberofDistrinctCommentators [(DistinctCommentator1)(DistinctCommentator2)...]`\n",
    " - Use a combiner here.\n",
    "\n",
    "* The third job will scan the input file and for each `CommentAuthor` it will construct a tuple that contains:\n",
    " - The number of comments that `CommentAuthor` has made to *all* posts, and\n",
    " - A list of the comments in the form `(PostAuthor,CommentDate)`. The list must be sorted in decreasing `CommentDate` order. Namely, the most recent comment must be placed at the top, followed by the older ones.\n",
    " - Example output: `CommentAuthor NumberofComments [(PostAuthor,Date)(PostAuthor,Date)(PostAuthor,Date)()...]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "\n",
    "**There will be a single deliverable, this notebook**. You will organize your answers according to the provided structure, which is identical to the example notebooks that were uploaded to the e-learning platform. **Please write your full name in both the notebook's filename and the notebook's title (first line of first cell)**.\n",
    "\n",
    "Then, upload the file in the e-learning platform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer to task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Python code for MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing task1.py\n"
     ]
    }
   ],
   "source": [
    "%%file task1.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "class PostAuthorComments(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        # Use csv library to correctly parse CSV lines, even if there are commas within fields\n",
    "        reader = csv.reader([line])\n",
    "        for row in reader:\n",
    "            post_author, comment_author, comment_date = row\n",
    "            yield post_author, (comment_author, comment_date)\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        # Sort comments by date descending and count them\n",
    "        sorted_values = sorted(values, key=lambda x: datetime.strptime(x[1], '%Y-%m-%d'), reverse=True)\n",
    "        num_comments = len(sorted_values)\n",
    "        yield key, (num_comments, list(sorted_values))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    PostAuthorComments.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Standalone execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db6ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python task1.py comments.csv > output1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Running in the Hadoop cluster in a fully/pseudo distributed mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0f6b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python task1.py -r hadoop hdfs:///path/to/comments.csv > output1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Copy the output file from HDFS to the local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1847d0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -get /path/to/hadoop/output/part-00000 output1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer to task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Python code for MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file task2.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class DistinctCommentAuthors(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_authors,\n",
    "                   combiner=self.combiner_unique_authors,\n",
    "                   reducer=self.reducer_count_unique_authors)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_authors(self, _, line):\n",
    "        try:\n",
    "            post_author, comment_author, _ = line.split(',')\n",
    "            yield post_author, comment_author\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    def combiner_unique_authors(self, post_author, comment_authors):\n",
    "        unique_authors = set(comment_authors)  # Remove duplicates in the combiner\n",
    "        for author in unique_authors:\n",
    "            yield post_author, author\n",
    "\n",
    "    def reducer_count_unique_authors(self, post_author, unique_authors):\n",
    "        unique_set = set(unique_authors)\n",
    "        yield post_author, len(unique_set)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    DistinctCommentAuthors.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Standalone execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8346bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python task2.py comments.csv > output2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Running in the Hadoop cluster in a fully/pseudo distributed mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7883b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python task2.py -r hadoop hdfs:///path/to/comments.csv > output2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Copy the output file from HDFS to the local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427ef6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -get /path/to/hadoop/output/part-00000 output2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer to task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Python code for MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file task3.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "class CommentAuthorDetails(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_posts,\n",
    "                   reducer=self.reducer_list_posts)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_posts(self, _, line):\n",
    "        try:\n",
    "            post_author, comment_author, comment_date = line.split(',')\n",
    "            yield comment_author, (post_author, comment_date)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    def reducer_list_posts(self, key, values):\n",
    "        posts_list = list(values)\n",
    "        posts_list.sort(key=lambda post: datetime.strptime(post[1], '%Y-%m-%d'), reverse=True)\n",
    "        yield key, (len(posts_list), posts_list)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    CommentAuthorDetails.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Standalone execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588f637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python task3.py comments.csv > output3.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Running in the Hadoop cluster in a fully/pseudo distributed mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48c647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python task3.py -r hadoop hdfs:///path/to/comments.csv > output3.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Copy the output file from HDFS to the local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c82c022",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -get /path/to/hadoop/output/part-00000 output3.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
