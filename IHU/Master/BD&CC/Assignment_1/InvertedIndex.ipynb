{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "modular-newspaper",
   "metadata": {},
   "source": [
    "# Inverted Index construction with MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-romantic",
   "metadata": {},
   "source": [
    "This code will firstly create a new Python file named <code>invertedIndex.py</code>. Then, it will write the following Python code into this file. The Python code of <code>invertedIndex.py</code> will be submitted to MapReduce.\n",
    "\n",
    "The following <code>Indexer</code> class inherits from the <code>MRJob</code> class. Therefore, it will run as a MapReduce job.\n",
    "\n",
    "Here we can overload (override) the <code>mapper</code>, <code>reducer</code>, and <code>combiner</code> methods and replace them by our code. More functions can be overloaded from the base class. A full list of the supported methods can be found <a href=\"https://mrjob.readthedocs.io/en/latest/job.html\" target=\"_blank\">here</a>.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-gregory",
   "metadata": {},
   "source": [
    "## 1. Document-level inverted file\n",
    "\n",
    "An inverted index is a data structure that for each term $t$ in the collection, it stores a (sorted) list of all document ids (postings) containing $t$.\n",
    "\n",
    "<hr width=\"100%\">\n",
    "\n",
    "<img src=\"img/doclevel.png\" style=\"width:60%\">\n",
    "\n",
    "<hr width=\"100%\">\n",
    "\n",
    "with this structure, a search engine can employ many algorithms to answer a user query. One of these algorithms is called Term-At-A-Time (TAAT):\n",
    "\n",
    "<hr width=\"100%\">\n",
    "\n",
    "<img src=\"img/taat.png\" style=\"width:60%\">\n",
    "\n",
    "<hr width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-wisdom",
   "metadata": {},
   "source": [
    "### 1.1. Python code for MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "technical-verse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting python/invertedIndexDL.py\n"
     ]
    }
   ],
   "source": [
    "%%file python/invertedIndexDL.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import string \n",
    "\n",
    "class Indexer(MRJob):\n",
    "    # map function\n",
    "    # Argument 1: self: the class itself (this)\n",
    "    # Argument 2: Input key to the map function (here:none)\n",
    "    # Argument 3: Input value to the map function (here:one line from the input file)\n",
    "    def mapper(self, _, line):\n",
    "        columns = line.replace('\"', '').split(\",\")\n",
    "\n",
    "        # The first column represents Document ID\n",
    "        DocumentID = int(columns[0])\n",
    "\n",
    "        # The fourth column represents the titles of the posts\n",
    "        title = columns[3]\n",
    "\n",
    "        # Perform punctuation removal\n",
    "        title = title.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # Perform casefolding\n",
    "        title = title.casefold()\n",
    "\n",
    "        # Extract the words from the title\n",
    "        words = title.split()\n",
    "\n",
    "        # Setup a dictionary for the terms of the document. The dictionaries do not contain duplicate keys.\n",
    "        # So we are sure that each word is emitted one time for each document.\n",
    "        doc_words = {}\n",
    "\n",
    "        for word in words:\n",
    "            if word not in doc_words:\n",
    "                doc_words[word] = word\n",
    "\n",
    "        for word in doc_words:\n",
    "            # Emit a (word, Document ID) pair\n",
    "            yield (word, DocumentID)\n",
    "\n",
    "    # reduce function\n",
    "    # Argument 1: self: the class itself (this)\n",
    "    # Argument 2: Input key to the reduce function (here: the key that was emitted by the mapper)\n",
    "    # Argument 3: Input value to the reduce function (here: a generator object; something like a\n",
    "    # sorted list of ALL values associated with the same key)\n",
    "    def reducer(self, word, docIDs):\n",
    "        inverted_list = [ docID for docID in docIDs ]\n",
    "        inverted_list.sort()\n",
    "        \n",
    "        yield(word, (len(inverted_list), inverted_list))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Indexer.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-receipt",
   "metadata": {},
   "source": [
    "### 1.2. Running in standalone mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dimensional-accounting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/bdccuser/.mrjob.conf\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/invertedIndexDL.bdccuser.20230425.085310.028785\n",
      "job output is in out/localout_iiDL\n",
      "Removing temp directory /tmp/invertedIndexDL.bdccuser.20230425.085310.028785...\n"
     ]
    }
   ],
   "source": [
    "!python3 python/invertedIndexDL.py data/posts.csv -o out/localout_iiDL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-petroleum",
   "metadata": {},
   "source": [
    "### 1.3. Running in the Hadoop cluster in a fully/pseudo distributed mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "twenty-labor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `out_iiDL': No such file or directory\n",
      "Using configs in /home/bdccuser/.mrjob.conf\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /home/hdoop/hadoop-3.2.1/bin...\n",
      "Found hadoop binary: /home/hdoop/hadoop-3.2.1/bin/hadoop\n",
      "Using Hadoop version 3.2.1\n",
      "Looking for Hadoop streaming jar in /home/hdoop/hadoop-3.2.1...\n",
      "Found Hadoop streaming jar: /home/hdoop/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar\n",
      "Creating temp directory /tmp/invertedIndexDL.bdccuser.20230425.085313.716021\n",
      "uploading working dir files to hdfs:///user/bdccuser/tmp/mrjob/invertedIndexDL.bdccuser.20230425.085313.716021/files/wd...\n",
      "Copying other local files to hdfs:///user/bdccuser/tmp/mrjob/invertedIndexDL.bdccuser.20230425.085313.716021/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar2728416951108793872/] [] /tmp/streamjob840659835583255781.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /127.0.0.1:8032\n",
      "  Connecting to ResourceManager at /127.0.0.1:8032\n",
      "  Error Launching job : Output directory hdfs://127.0.0.1:9000/user/bdccuser/out_iiDL22 already exists\n",
      "  Streaming Command Failed!\n",
      "Attempting to fetch counters from logs...\n",
      "Can't fetch history log; missing job ID\n",
      "No counters found\n",
      "Scanning logs for probable cause of failure...\n",
      "Can't fetch history log; missing job ID\n",
      "Can't fetch task logs; missing application ID\n",
      "Step 1 of 1 failed: Command '['/home/hdoop/hadoop-3.2.1/bin/hadoop', 'jar', '/home/hdoop/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar', '-files', 'hdfs:///user/bdccuser/tmp/mrjob/invertedIndexDL.bdccuser.20230425.085313.716021/files/wd/invertedIndexDL.py#invertedIndexDL.py,hdfs:///user/bdccuser/tmp/mrjob/invertedIndexDL.bdccuser.20230425.085313.716021/files/wd/mrjob.zip#mrjob.zip,hdfs:///user/bdccuser/tmp/mrjob/invertedIndexDL.bdccuser.20230425.085313.716021/files/wd/setup-wrapper.sh#setup-wrapper.sh', '-input', 'hdfs:///user/bdccuser/tmp/mrjob/invertedIndexDL.bdccuser.20230425.085313.716021/files/posts.csv', '-output', 'hdfs:///user/bdccuser/out_iiDL22', '-mapper', '/bin/sh -ex setup-wrapper.sh python3 invertedIndexDL.py --step-num=0 --mapper', '-reducer', '/bin/sh -ex setup-wrapper.sh python3 invertedIndexDL.py --step-num=0 --reducer']' returned non-zero exit status 1280.\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r out_iiDL\n",
    "!python3 python/invertedIndexDL.py -r hadoop data/posts.csv -o out_iiDL22\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-whale",
   "metadata": {},
   "source": [
    "### 1.4. Copy the output file from HDFS to the local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "vital-picture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-25 11:53:34,201 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -copyToLocal -f out_iiDL22 /home/bdccuser/notebooks/mapreduce/out/out_iiDL22\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-basketball",
   "metadata": {},
   "source": [
    "## 2. Term-level inverted file (with frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-sustainability",
   "metadata": {},
   "source": [
    "A document-level inverted index can only answer boolean queries, similarly to a database. More specifically, it can return all documents that satisfy a particular query, but it tells us nothing about the quality of the documents, or their relevance to the user query.\n",
    "\n",
    "To overcome this problem, a term-level inverted file also contains the frequency of each word in a document (known as term-document frequency). This value along with some other statistics, provides us with the ability to perform ranked retrieval. This is a procedure where we are not particularly interested in retrieving <b>all</b> the candidate documents, but only the <b>best</b>-$k$ among them.\n",
    "\n",
    "\n",
    "<hr width=\"100%\">\n",
    "\n",
    "<img src=\"img/termlevel.png\" style=\"width:60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-emission",
   "metadata": {},
   "source": [
    "### 2.1. Python code for MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "legendary-american",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting python/invertedIndexTL.py\n"
     ]
    }
   ],
   "source": [
    "%%file python/invertedIndexTL.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import string \n",
    "\n",
    "class Indexer(MRJob):\n",
    "    # map function\n",
    "    # Argumnet 1: self: the class itself (this)\n",
    "    # Argumnet 2: Input key to the map function (here:none)\n",
    "    # Argumnet 3: Input value to the map function (here:one line from the input file)\n",
    "    def mapper(self, _, line):\n",
    "        columns = line.replace('\"', '').split(\",\")\n",
    "\n",
    "        # The first column represents Document ID\n",
    "        DocumentID = int(columns[0])\n",
    "\n",
    "        # The fourth column represents the titles of the posts\n",
    "        title = columns[3]\n",
    "\n",
    "        # Perform punctuation removal\n",
    "        title = title.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # Perform casefolding\n",
    "        title = title.casefold()\n",
    "\n",
    "        # Extract the words from the title\n",
    "        words = title.split()\n",
    "\n",
    "        # Setup a dictionary for the terms of the document. The dictionaries do not contain duplicate keys.\n",
    "        # So we are sure that each word is emitted one time for each document.\n",
    "        doc_words = {}\n",
    "\n",
    "        for word in words:\n",
    "            if word in doc_words:\n",
    "                doc_words[word] = doc_words[word] + 1\n",
    "            else:\n",
    "                doc_words[word] = 1\n",
    "\n",
    "\n",
    "        for word in doc_words:\n",
    "            # Emit a (word, Document ID) pair\n",
    "            yield (word, (DocumentID, doc_words[word]))\n",
    "\n",
    "\n",
    "    # reduce function\n",
    "    # Argumnet 1: self: the class itself (this)\n",
    "    # Argumnet 2: Input key to the reduce function (here: the key that was emitted by the mapper)\n",
    "    # Argumnet 3: Input value to the reduce function (here: a generator object; something like a\n",
    "    # sorted list of ALL values associated with the same key)\n",
    "    def reducer(self, word, postings):\n",
    "        inv_list = [ posting for posting in postings ]\n",
    "\n",
    "        # Here the inv_list does not contain duplicates\n",
    "        yield(word, (len(inv_list), inv_list))\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    Indexer.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-lying",
   "metadata": {},
   "source": [
    "The above procedure constructs a document-level inverted index with MapReduce on the titles of some blog posts. The procedure is as follows:\n",
    "\n",
    "<ol>\n",
    "    <li>Map over all documents</li>\n",
    "    <li>Emit the word as a key and the document ID as the value</li>\n",
    "    <li>Sort/shuffle: group postings by term (automatic)</li>\n",
    "    <li>Reduce: Gather and sort the postings (e.g., by docid)</li>\n",
    "    <li>Write postings to disk</li>\n",
    "    <li>MapReduce does all the heavy lifting!</li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-metropolitan",
   "metadata": {},
   "source": [
    "### 2.2. Running in standalone mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "higher-parallel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/bdccuser/.mrjob.conf\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/invertedIndexTL.bdccuser.20230425.085334.834225\n",
      "job output is in out/localout_iiTL\n",
      "Removing temp directory /tmp/invertedIndexTL.bdccuser.20230425.085334.834225...\n"
     ]
    }
   ],
   "source": [
    "!python3 python/invertedIndexTL.py data/posts.csv -o out/localout_iiTL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-assistant",
   "metadata": {},
   "source": [
    "### 2.3. Running in the Hadoop cluster in a fully/pseudo distributed mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "demanding-empty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted out_iiTL\n",
      "Using configs in /home/bdccuser/.mrjob.conf\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /home/hdoop/hadoop-3.2.1/bin...\n",
      "Found hadoop binary: /home/hdoop/hadoop-3.2.1/bin/hadoop\n",
      "Using Hadoop version 3.2.1\n",
      "Looking for Hadoop streaming jar in /home/hdoop/hadoop-3.2.1...\n",
      "Found Hadoop streaming jar: /home/hdoop/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar\n",
      "Creating temp directory /tmp/invertedIndexTL.bdccuser.20230425.085337.728660\n",
      "uploading working dir files to hdfs:///user/bdccuser/tmp/mrjob/invertedIndexTL.bdccuser.20230425.085337.728660/files/wd...\n",
      "Copying other local files to hdfs:///user/bdccuser/tmp/mrjob/invertedIndexTL.bdccuser.20230425.085337.728660/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar4389966928610581573/] [] /tmp/streamjob1521693950853296767.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /127.0.0.1:8032\n",
      "  Connecting to ResourceManager at /127.0.0.1:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/bdccuser/.staging/job_1682412739464_0001\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  Total input files to process : 1\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  number of splits:2\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  Submitting tokens for job: job_1682412739464_0001\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1682412739464_0001\n",
      "  The url to track the job: http://bdcc:8088/proxy/application_1682412739464_0001/\n",
      "  Running job: job_1682412739464_0001\n",
      "  Job job_1682412739464_0001 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1682412739464_0001 completed successfully\n",
      "  Output directory: hdfs:///user/bdccuser/out_iiTL\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=26854284\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1390017\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2093482\n",
      "\t\tFILE: Number of bytes written=4881115\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=26854606\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=1390017\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=14372864\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3656704\n",
      "\t\tTotal time spent by all map tasks (ms)=14036\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=14036\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3571\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3571\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=14036\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3571\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2270\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=190\n",
      "\t\tInput split bytes=322\n",
      "\t\tMap input records=17831\n",
      "\t\tMap output bytes=1888778\n",
      "\t\tMap output materialized bytes=2093488\n",
      "\t\tMap output records=102349\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=227794944\n",
      "\t\tPeak Map Virtual memory (bytes)=2484076544\n",
      "\t\tPeak Reduce Physical memory (bytes)=126054400\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2488553472\n",
      "\t\tPhysical memory (bytes) snapshot=581308416\n",
      "\t\tReduce input groups=14070\n",
      "\t\tReduce input records=102349\n",
      "\t\tReduce output records=14070\n",
      "\t\tReduce shuffle bytes=2093488\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=204698\n",
      "\t\tTotal committed heap usage (bytes)=398663680\n",
      "\t\tVirtual memory (bytes) snapshot=7456706560\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/bdccuser/out_iiTL\n",
      "Removing HDFS temp directory hdfs:///user/bdccuser/tmp/mrjob/invertedIndexTL.bdccuser.20230425.085337.728660...\n",
      "Removing temp directory /tmp/invertedIndexTL.bdccuser.20230425.085337.728660...\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r out_iiTL\n",
    "!python3 python/invertedIndexTL.py -r hadoop data/posts.csv -o out_iiTL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-square",
   "metadata": {},
   "source": [
    "### 2.4. Copy the output file from HDFS to the local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "becoming-surname",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-25 11:54:22,905 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -copyToLocal -f out_iiTL /home/bdccuser/notebooks/mapreduce/out/out_iiTL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-stomach",
   "metadata": {},
   "source": [
    "## 3. Positional Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-bridal",
   "metadata": {},
   "source": [
    "A positional index allows the introduction of more sophisitcated ranking models and broadens the types of queries that can be answered by a system. For example, the inclusion of the position of a term in a document allows the processing of \"exact phrase\" queries. It also supports ranking models that take into consideration the notion of term-proximity to identify the best documents.\n",
    "\n",
    "<hr width=\"100%\">\n",
    "\n",
    "<img src=\"img/positional.png\" style=\"width:60%\">\n",
    "\n",
    "<hr width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-venezuela",
   "metadata": {},
   "source": [
    "### 3.1. Python code for MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "active-elizabeth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting python/invertedIndexPI.py\n"
     ]
    }
   ],
   "source": [
    "%%file python/invertedIndexPI.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import string \n",
    "\n",
    "class Indexer(MRJob):\n",
    "    # map function\n",
    "    # Argumnet 1: self: the class itself (this)\n",
    "    # Argumnet 2: Input key to the map function (here:none)\n",
    "    # Argumnet 3: Input value to the map function (here:one line from the input file)\n",
    "    def mapper(self, _, line):\n",
    "        columns = line.replace('\"', '').split(\",\")\n",
    "\n",
    "        # The first column represents Document ID\n",
    "        DocumentID = int(columns[0])\n",
    "\n",
    "        # The fourth column represents the titles of the posts\n",
    "        title = columns[3]\n",
    "\n",
    "        # Perform punctuation removal\n",
    "        title = title.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # Perform casefolding\n",
    "        title = title.casefold()\n",
    "\n",
    "        # Extract the words from the title\n",
    "        words = title.split()\n",
    "\n",
    "        # Setup a dictionary for the terms of the document\n",
    "        doc_words = {}\n",
    "        position = 0\n",
    "\n",
    "        for word in words:\n",
    "            position = position + 1\n",
    "\n",
    "            if word in doc_words:\n",
    "                doc_words[word]['freq'] = doc_words[word]['freq'] + 1\n",
    "                doc_words[word]['pos'].append(position)\n",
    "\n",
    "            else:\n",
    "                doc_words[word] = {'freq': 1, 'pos': [position] }\n",
    "\n",
    "\n",
    "        for word in doc_words:\n",
    "            # Emit a (word, Document ID) pair\n",
    "            yield (word, (DocumentID, doc_words[word]['freq'], doc_words[word]['pos']))\n",
    "\n",
    "\n",
    "    # reduce function\n",
    "    # Argumnet 1: self: the class itself (this)\n",
    "    # Argumnet 2: Input key to the reduce function (here: the key that was emitted by the mapper)\n",
    "    # Argumnet 3: Input value to the reduce function (here: a generator object; something like a\n",
    "    # sorted list of ALL values associated with the same key)\n",
    "    def reducer(self, word, postings):\n",
    "        inv_list = [ posting for posting in postings ]\n",
    "        yield(word, (len(inv_list), inv_list))\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    Indexer.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-greek",
   "metadata": {},
   "source": [
    "### 3.2. Running in standalone mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "affiliated-hybrid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/bdccuser/.mrjob.conf\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/invertedIndexPI.bdccuser.20230425.085423.638200\n",
      "job output is in out/localout_iiPI\n",
      "Removing temp directory /tmp/invertedIndexPI.bdccuser.20230425.085423.638200...\n"
     ]
    }
   ],
   "source": [
    "!python3 python/invertedIndexPI.py data/posts.csv -o out/localout_iiPI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-arrangement",
   "metadata": {},
   "source": [
    "### 3.3. Running in the Hadoop cluster in a fully/pseudo distributed mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "neutral-november",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted out_iiPI\n",
      "Using configs in /home/bdccuser/.mrjob.conf\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /home/hdoop/hadoop-3.2.1/bin...\n",
      "Found hadoop binary: /home/hdoop/hadoop-3.2.1/bin/hadoop\n",
      "Using Hadoop version 3.2.1\n",
      "Looking for Hadoop streaming jar in /home/hdoop/hadoop-3.2.1...\n",
      "Found Hadoop streaming jar: /home/hdoop/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar\n",
      "Creating temp directory /tmp/invertedIndexPI.bdccuser.20230425.085426.509014\n",
      "uploading working dir files to hdfs:///user/bdccuser/tmp/mrjob/invertedIndexPI.bdccuser.20230425.085426.509014/files/wd...\n",
      "Copying other local files to hdfs:///user/bdccuser/tmp/mrjob/invertedIndexPI.bdccuser.20230425.085426.509014/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar6428007698647658147/] [] /tmp/streamjob4462153889864768540.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /127.0.0.1:8032\n",
      "  Connecting to ResourceManager at /127.0.0.1:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/bdccuser/.staging/job_1682412739464_0002\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  Total input files to process : 1\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  Caught exception\n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1257)\n",
      "\tat java.lang.Thread.join(Thread.java:1331)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  number of splits:2\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  Submitting tokens for job: job_1682412739464_0002\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1682412739464_0002\n",
      "  The url to track the job: http://bdcc:8088/proxy/application_1682412739464_0002/\n",
      "  Running job: job_1682412739464_0002\n",
      "  Job job_1682412739464_0002 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1682412739464_0002 completed successfully\n",
      "  Output directory: hdfs:///user/bdccuser/out_iiPI\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=26854284\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1906827\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2610292\n",
      "\t\tFILE: Number of bytes written=5914735\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=26854606\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=1906827\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=18660352\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=5238784\n",
      "\t\tTotal time spent by all map tasks (ms)=18223\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18223\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5116\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5116\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=18223\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5116\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2800\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=369\n",
      "\t\tInput split bytes=322\n",
      "\t\tMap input records=17831\n",
      "\t\tMap output bytes=2405588\n",
      "\t\tMap output materialized bytes=2610298\n",
      "\t\tMap output records=102349\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=227704832\n",
      "\t\tPeak Map Virtual memory (bytes)=2484076544\n",
      "\t\tPeak Reduce Physical memory (bytes)=124624896\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2488467456\n",
      "\t\tPhysical memory (bytes) snapshot=577273856\n",
      "\t\tReduce input groups=14070\n",
      "\t\tReduce input records=102349\n",
      "\t\tReduce output records=14070\n",
      "\t\tReduce shuffle bytes=2610298\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=204698\n",
      "\t\tTotal committed heap usage (bytes)=398663680\n",
      "\t\tVirtual memory (bytes) snapshot=7456620544\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/bdccuser/out_iiPI\n",
      "Removing HDFS temp directory hdfs:///user/bdccuser/tmp/mrjob/invertedIndexPI.bdccuser.20230425.085426.509014...\n",
      "Removing temp directory /tmp/invertedIndexPI.bdccuser.20230425.085426.509014...\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r out_iiPI\n",
    "!python3 python/invertedIndexPI.py -r hadoop data/posts.csv -o out_iiPI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-protection",
   "metadata": {},
   "source": [
    "### 3.4. Copy the output file from HDFS to the local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "generous-satellite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-25 11:55:15,711 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -copyToLocal -f out_iiPI /home/bdccuser/notebooks/mapreduce/out/out_iiPI\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
