{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f5f8d37",
   "metadata": {},
   "source": [
    "# Christos Stylidis\n",
    "# Assignment 2: Spark - Graph Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34da2fa6",
   "metadata": {},
   "source": [
    "## Input file description\n",
    "\n",
    "You are given the `p2p-Gnutella08.txt` input file. It is a Comma Separated Values (CSV) file that stores a snapshot of the Gnutella P2P network from August 8 2002. The snapshot is provided in the form of a directed graph, where each row represents a connection between two servers.\n",
    "\n",
    "`ServerFrom_ID,ServerTo_ID`\n",
    "\n",
    "The tuple contains two fields (columns):\n",
    "\n",
    "* `ServerFrom_ID`: the ID of the connection's source server.\n",
    "* `ServerTo_ID`: the ID of the connection's target server.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5b5e34",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "You will write 4 Spark jobs **here** by using PySpark:\n",
    "\n",
    "* The first job will construct the graph's adjacency lists. That is, for each server ID, the task will build a list of all the incoming connections in the form `ServerID, [ServerFrom1_ID, ServerFrom2_ID, ...]`\n",
    "\n",
    "* The second job will count the number of nodes and edges in the graph.\n",
    "\n",
    "* The third job will count the number of nodes for each outdegree. That is, how many nodes have no outgoing edges, how many have one outgoing edge, how many have two outgoing edges, and so on?\n",
    "\n",
    "* The fourth job will count the number of nodes for each indegree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f90eedd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Deliverables\n",
    "\n",
    "**There will be a single deliverable, this notebook**. You will organize your answers according to the provided structure, which is identical to the example notebooks that were uploaded to the e-learning platform. **Please write your full name in both the notebook's filename and the notebook's title (first line of first cell)**.\n",
    "\n",
    "Then, upload the file to the e-learning platform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788bc87c",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af86e6d8-bf54-48b7-b6ca-31cbb8bd65f3",
   "metadata": {},
   "source": [
    "## Job 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10eebeb8-b9ce-4bbc-b5af-39ac175307fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hadoop fs -put p2p-Gnutella08.txt /user/hadoop/p2p-Gnutella08.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25aae0f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', {'0'})\n",
      "('2', {'0', '3192', '553', '3552', '2546', '4752'})\n",
      "('3', {'0', '2032', '3820', '2796', '2901', '5267', '3459', '992', '180', '1487', '1784', '5654', '4160', '174', '3693', '3495', '5724', '1046', '5792', '2084', '3526', '2176', '3077', '3515', '3026', '2873', '3890', '3551', '2490', '1676', '2822', '3544', '1454', '4260', '2252', '762', '15', '2465', '1199', '4003', '2695', '820', '1082', '504', '6132', '3042', '409', '2699', '4553', '4002', '1477', '2241', '1564', '36', '2546', '3660', '120', '1059', '4405', '3505', '3577', '123', '1541', '4179', '2313', '3213', '366', '1161', '4482', '2733', '507', '2986', '30', '580', '1739', '1527', '3176'})\n",
      "('4', {'339', '481', '6092', '0', '4211', '4275', '4306', '1934', '2146', '427', '1907', '5928', '195', '4382', '1458', '4353', '2881', '2066', '665', '5809', '3937', '1113', '6175', '720', '423', '830', '371', '4598', '3515', '1477', '434', '263', '4297', '4470', '1784', '4445', '2350', '3856', '553', '1586', '1866', '2621', '3061', '5726', '3040', '1723', '2822', '4532', '352', '2670', '1393', '1135', '5338', '3373', '179', '2751', '449', '5359', '367'})\n",
      "('5', {'339', '1885', '4276', '1245', '0', '6174', '4111', '4810', '1934', '5988', '2146', '5386', '132', '4900', '1129', '1458', '4353', '3553', '3734', '924', '4217', '3276', '665', '2770', '3213', '820', '2796', '4636', '2374', '1161', '1259', '720', '2819', '3269', '4279', '2501', '4197', '2217', '2879', '434', '4705', '2549', '30', '937', '2350', '1603', '1193', '4755', '155', '1537', '1647', '505', '1676', '3040', '4399', '176', '856', '4752', '174', '4988', '2463', '1393', '2047', '149', '752', '2193', '930', '1494', '2020', '2797', '1409', '367', '4472'})\n",
      "('6', {'0'})\n",
      "('7', {'339', '6174', '4276', '0', '2046', '298', '2750', '2861', '1907', '558', '5281', '1129', '3213', '3919', '924', '2606', '5418', '3465', '1082', '5809', '2052', '2796', '820', '146', '1904', '371', '434', '4470', '2779', '3403', '937', '3856', '4755', '141', '1676', '2096', '1723', '2822', '856', '4160', '174', '83', '123', '3059', '930', '3579', '1494', '5724', '2252', '4112', '367'})\n",
      "('8', {'144', '6092', '0', '5893', '6174', '298', '3192', '4423', '421', '2750', '2084', '2790', '1485', '4419', '961', '4433', '3526', '4900', '4003', '3213', '3883', '3849', '3276', '2424', '2712', '1082', '5809', '425', '2796', '3820', '1161', '2901', '989', '167', '720', '147', '2733', '4482', '279', '485', '3979', '2544', '3233', '4751', '6181', '2546', '1059', '4577', '30', '5831', '1603', '605', '1586', '2666', '3465', '4164', '1676', '327', '5532', '1417', '4752', '5515', '2767', '83', '3059', '1706', '981', '2252', '4112', '4174', '2770', '586', '5452', '3176'})\n",
      "('9', {'2046', '0', '1704', '1473', '127', '5988', '2146', '1414', '1485', '132', '665', '3452', '4456', '2243', '5950', '1412', '371', '3979', '5267', '2217', '3309', '4297', '6077', '753', '2627', '1310', '3043', '1603', '1537', '5726', '4103', '4160', '1394', '4016', '4988', '628', '2767', '83', '3577', '3081', '3373', '179', '4257', '369', '4260', '646', '4636', '1732', '238'})\n",
      "('10', {'0', '4211', '149', '2146', '2679', '2056', '2290'})\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def initialize_spark(app_name):\n",
    "    \"\"\"Initialize and return a Spark session.\"\"\"\n",
    "    return SparkSession.builder.appName(app_name).getOrCreate()\n",
    "\n",
    "def read_data(file_path, sc):\n",
    "    \"\"\"Read the input file and return RDD of lines.\"\"\"\n",
    "    return sc.textFile(file_path)\n",
    "\n",
    "def parse_edge(line):\n",
    "    \"\"\"Parse an edge from a line, with validation.\"\"\"\n",
    "    tokens = line.split(\",\")\n",
    "    if len(tokens) != 2 or not tokens[0].isdigit() or not tokens[1].isdigit():\n",
    "        return None\n",
    "    return tokens[1], tokens[0]\n",
    "\n",
    "def create_adjacency_list(edges):\n",
    "    \"\"\"Transform edges into adjacency lists.\"\"\"\n",
    "    return edges.filter(lambda x: x is not None).groupByKey().mapValues(set)\n",
    "\n",
    "def main():\n",
    "    # Initialize Spark session\n",
    "    spark = initialize_spark(\"GraphAnalysisOptimized\")\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Read the data file\n",
    "    file_path = \"p2p-Gnutella08.txt\"\n",
    "    lines = read_data(file_path, sc)\n",
    "\n",
    "    # Parse the data into edges\n",
    "    edges = lines.map(parse_edge)\n",
    "\n",
    "    # Transform into adjacency lists\n",
    "    adjacency_lists = create_adjacency_list(edges)\n",
    "\n",
    "    # Output the results\n",
    "    results = adjacency_lists.take(10)\n",
    "    for result in results:\n",
    "        print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099abb4b-d8ae-4a9c-a76d-5cc925a683f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f4c1b5c-d015-4927-adf6-f8122a7f3ea8",
   "metadata": {},
   "source": [
    "## Job 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c151037b-f71a-461f-84da-1ece201ed115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of nodes (servers): 6301\n",
      "Total number of edges (connections): 20777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"GraphAnalysis_Count\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Read the data file\n",
    "file_path = \"p2p-Gnutella08.txt\"  # Update with the correct path\n",
    "lines = sc.textFile(file_path)\n",
    "\n",
    "# Parse the data\n",
    "edges = lines.map(lambda line: line.split(\",\")).map(lambda tokens: (tokens[0], tokens[1]))\n",
    "\n",
    "# Counting nodes\n",
    "nodes_from = edges.map(lambda x: x[0])\n",
    "nodes_to = edges.map(lambda x: x[1])\n",
    "unique_nodes = nodes_from.union(nodes_to).distinct().count()\n",
    "\n",
    "# Counting edges\n",
    "total_edges = edges.count()\n",
    "\n",
    "print(f\"Total number of nodes (servers): {unique_nodes}\")\n",
    "print(f\"Total number of edges (connections): {total_edges}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ef849-94cc-42e7-9ed3-45d87ab894d3",
   "metadata": {},
   "source": [
    "## Job 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5dc8efc-c141-439d-8fe8-e369da4fa3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes with outdegree 1: 2452\n",
      "Nodes with outdegree 6: 227\n",
      "Nodes with outdegree 77: 2\n",
      "Nodes with outdegree 59: 1\n",
      "Nodes with outdegree 73: 2\n",
      "Nodes with outdegree 51: 1\n",
      "Nodes with outdegree 74: 1\n",
      "Nodes with outdegree 49: 1\n",
      "Nodes with outdegree 7: 144\n",
      "Nodes with outdegree 5: 333\n",
      "Nodes with outdegree 4: 559\n",
      "Nodes with outdegree 12: 23\n",
      "Nodes with outdegree 3: 868\n",
      "Nodes with outdegree 2: 1287\n",
      "Nodes with outdegree 70: 3\n",
      "Nodes with outdegree 11: 29\n",
      "Nodes with outdegree 54: 1\n",
      "Nodes with outdegree 85: 1\n",
      "Nodes with outdegree 50: 1\n",
      "Nodes with outdegree 71: 3\n",
      "Nodes with outdegree 60: 3\n",
      "Nodes with outdegree 82: 1\n",
      "Nodes with outdegree 81: 4\n",
      "Nodes with outdegree 83: 1\n",
      "Nodes with outdegree 19: 2\n",
      "Nodes with outdegree 66: 2\n",
      "Nodes with outdegree 14: 13\n",
      "Nodes with outdegree 52: 1\n",
      "Nodes with outdegree 20: 4\n",
      "Nodes with outdegree 13: 19\n",
      "Nodes with outdegree 67: 3\n",
      "Nodes with outdegree 21: 2\n",
      "Nodes with outdegree 57: 1\n",
      "Nodes with outdegree 38: 1\n",
      "Nodes with outdegree 47: 2\n",
      "Nodes with outdegree 30: 1\n",
      "Nodes with outdegree 10: 37\n",
      "Nodes with outdegree 87: 1\n",
      "Nodes with outdegree 61: 1\n",
      "Nodes with outdegree 86: 1\n",
      "Nodes with outdegree 32: 2\n",
      "Nodes with outdegree 8: 76\n",
      "Nodes with outdegree 15: 8\n",
      "Nodes with outdegree 27: 1\n",
      "Nodes with outdegree 9: 70\n",
      "Nodes with outdegree 62: 2\n",
      "Nodes with outdegree 18: 2\n",
      "Nodes with outdegree 35: 1\n",
      "Nodes with outdegree 63: 1\n",
      "Nodes with outdegree 72: 2\n",
      "Nodes with outdegree 44: 1\n",
      "Nodes with outdegree 79: 1\n",
      "Nodes with outdegree 69: 2\n",
      "Nodes with outdegree 55: 1\n",
      "Nodes with outdegree 56: 2\n",
      "Nodes with outdegree 22: 1\n",
      "Nodes with outdegree 78: 1\n",
      "Nodes with outdegree 25: 1\n",
      "Nodes with outdegree 41: 1\n",
      "Nodes with outdegree 91: 1\n",
      "Nodes with outdegree 33: 1\n",
      "Nodes with outdegree 31: 2\n",
      "Nodes with outdegree 23: 1\n",
      "Nodes with outdegree 16: 1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def initialize_spark(app_name):\n",
    "    \"\"\"Initialize and return a Spark session.\"\"\"\n",
    "    return SparkSession.builder.appName(app_name).getOrCreate()\n",
    "\n",
    "def read_data(file_path, sc):\n",
    "    \"\"\"Read the input file and return RDD of lines.\"\"\"\n",
    "    return sc.textFile(file_path)\n",
    "\n",
    "def parse_edge(line):\n",
    "    \"\"\"Parse an edge from a line, with validation.\"\"\"\n",
    "    tokens = line.split(\",\")\n",
    "    if len(tokens) != 2 or not tokens[0].isdigit() or not tokens[1].isdigit():\n",
    "        return None\n",
    "    return tokens[0], tokens[1]\n",
    "\n",
    "def count_outdegree(nodes_to):\n",
    "    \"\"\"Count outdegree for each node.\"\"\"\n",
    "    return nodes_to.map(lambda x: (x[0], 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "def count_nodes_for_outdegree(outdegree_counts):\n",
    "    \"\"\"Count the number of nodes for each outdegree.\"\"\"\n",
    "    return outdegree_counts.map(lambda x: (x[1], 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "def main():\n",
    "    # Initialize Spark session\n",
    "    spark = initialize_spark(\"GraphAnalysisOutdegree\")\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Read the data file\n",
    "    file_path = \"p2p-Gnutella08.txt\"\n",
    "    lines = read_data(file_path, sc)\n",
    "\n",
    "    # Parse the data into edges\n",
    "    edges = lines.map(parse_edge)\n",
    "\n",
    "    # Count outdegree for each node\n",
    "    nodes_to = edges.map(lambda x: (x[1], None))  # Only consider destination nodes\n",
    "    outdegree_counts = count_outdegree(nodes_to)\n",
    "\n",
    "    # Count the number of nodes for each outdegree\n",
    "    nodes_for_outdegree = count_nodes_for_outdegree(outdegree_counts)\n",
    "\n",
    "    # Output the results\n",
    "    results = nodes_for_outdegree.collect()\n",
    "    for result in results:\n",
    "        print(f\"Nodes with outdegree {result[0]}: {result[1]}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58995b7-301b-4188-a16f-c67c3519d9b8",
   "metadata": {},
   "source": [
    "## Job 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db577ac-abdd-40a0-9a1c-ae7185d1f29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes with indegree 10: 1531\n",
      "Nodes with indegree 9: 372\n",
      "Nodes with indegree 1: 294\n",
      "Nodes with indegree 3: 16\n",
      "Nodes with indegree 7: 9\n",
      "Nodes with indegree 2: 28\n",
      "Nodes with indegree 5: 107\n",
      "Nodes with indegree 8: 44\n",
      "Nodes with indegree 6: 10\n",
      "Nodes with indegree 4: 28\n",
      "Nodes with indegree 13: 2\n",
      "Nodes with indegree 14: 2\n",
      "Nodes with indegree 18: 2\n",
      "Nodes with indegree 25: 1\n",
      "Nodes with indegree 12: 3\n",
      "Nodes with indegree 29: 1\n",
      "Nodes with indegree 19: 1\n",
      "Nodes with indegree 47: 1\n",
      "Nodes with indegree 34: 1\n",
      "Nodes with indegree 24: 1\n",
      "Nodes with indegree 22: 1\n",
      "Nodes with indegree 28: 1\n",
      "Nodes with indegree 11: 1\n",
      "Nodes with indegree 17: 4\n",
      "Nodes with indegree 46: 1\n",
      "Nodes with indegree 48: 1\n",
      "Nodes with indegree 31: 1\n",
      "Nodes with indegree 41: 1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def initialize_spark(app_name):\n",
    "    \"\"\"Initialize and return a Spark session.\"\"\"\n",
    "    return SparkSession.builder.appName(app_name).getOrCreate()\n",
    "\n",
    "def read_data(file_path, sc):\n",
    "    \"\"\"Read the input file and return RDD of lines.\"\"\"\n",
    "    return sc.textFile(file_path)\n",
    "\n",
    "def parse_edge(line):\n",
    "    \"\"\"Parse an edge from a line, with validation.\"\"\"\n",
    "    tokens = line.split(\",\")\n",
    "    if len(tokens) != 2 or not tokens[0].isdigit() or not tokens[1].isdigit():\n",
    "        return None\n",
    "    return tokens[1], tokens[0]\n",
    "\n",
    "def count_indegree(nodes_from):\n",
    "    \"\"\"Count indegree for each node.\"\"\"\n",
    "    return nodes_from.map(lambda x: (x[0], 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "def count_nodes_for_indegree(indegree_counts):\n",
    "    \"\"\"Count the number of nodes for each indegree.\"\"\"\n",
    "    return indegree_counts.map(lambda x: (x[1], 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "def main():\n",
    "    # Initialize Spark session\n",
    "    spark = initialize_spark(\"GraphAnalysisIndegree\")\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Read the data file\n",
    "    file_path = \"p2p-Gnutella08.txt\"\n",
    "    lines = read_data(file_path, sc)\n",
    "\n",
    "    # Parse the data into edges\n",
    "    edges = lines.map(parse_edge)\n",
    "\n",
    "    # Count indegree for each node\n",
    "    nodes_from = edges.map(lambda x: (x[1], None))  # Only consider source nodes\n",
    "    indegree_counts = count_indegree(nodes_from)\n",
    "\n",
    "    # Count the number of nodes for each indegree\n",
    "    nodes_for_indegree = count_nodes_for_indegree(indegree_counts)\n",
    "\n",
    "    # Output the results\n",
    "    results = nodes_for_indegree.collect()\n",
    "    for result in results:\n",
    "        print(f\"Nodes with indegree {result[0]}: {result[1]}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37333df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resources\n",
    "\"https://spark.apache.org/docs/latest/graphx-programming-guide.html\"\"\n",
    "\"https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations\"\n",
    "\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a843b4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
